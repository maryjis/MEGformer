{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f136622e-9621-4fe2-9831-e05d37caf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: local-attention in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from local-attention) (0.7.0)\n",
      "Requirement already satisfied: torch in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from local-attention) (2.1.2)\n",
      "Requirement already satisfied: filelock in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from torch->local-attention) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from torch->local-attention) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from torch->local-attention) (1.12)\n",
      "Requirement already satisfied: networkx in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from torch->local-attention) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from torch->local-attention) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from torch->local-attention) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from jinja2->torch->local-attention) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/zubrikhina/anaconda3/envs/bm/lib/python3.8/site-packages (from sympy->torch->local-attention) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install local-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d954ed3f-d651-4b6d-82a1-ecc6fa1c103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from local_attention import LocalAttention\n",
    "from torch import Tensor, nn\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644b2cfe-ad0f-43f1-b0f1-5e66f885ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch =torch.rand(size=(256, 207, 361))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d96239-8755-4b3b-9f43-06febc6945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = torch.randn(2, 8, 2048, 64)\n",
    "k = torch.randn(2, 8, 2048, 64)\n",
    "v = torch.randn(2, 8, 2048, 64)\n",
    "\n",
    "attn = LocalAttention(\n",
    "    dim = 64,                # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "    window_size = 512,       # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "    causal = True,           # auto-regressive or not\n",
    "    look_backward = 1,       # each window looks at the window before\n",
    "    look_forward = 0,        # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "    dropout = 0.1,           # post-attention dropout\n",
    "    exact_windowsize = False # if this is set to true, in the causal setting, each query will see at maximum the number of keys equal to the window size\n",
    ")\n",
    "\n",
    "mask = torch.ones(2, 2048).bool()\n",
    "out = attn(q, k, v, mask = mask) # (2, 8, 2048, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dd8fb1-c21c-4a86-9c72-f199a6ce2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_attention import LocalTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6010ab4-75dc-4d4e-9160-a0582e8945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalTransformer(\n",
    "    num_tokens = 256,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    max_seq_len = 8192,\n",
    "    causal = True,\n",
    "    local_attn_window_size = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee813b0-87ba-4294-b208-b8b9cdd8c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerConfig, LongformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309945b2-fe8f-425b-9d8a-7039bc7548e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.longformer.configuration_longformer.LongformerConfig"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LongformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3042f40b-4452-4e3b-96e7-f86a32ccb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = LongformerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6921cd-e0fd-4260-813e-212e099608aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": 512,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"onnx_export\": false,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72ca4f3-58b6-471b-8a4c-cc367f880921",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongformerModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc139ad0-7a53-46ea-a8d1-fb6ee0240ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076857f4-c683-4fd9-8ecf-91dfbda84488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.longformer.modeling_longformer import LongformerLayer\n",
    "from transformers.models.longformer.configuration_longformer import LongformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d81495f1-e788-47ff-96ad-b2ff33d6e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"attention_window\": [\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512,\n",
    "    512\n",
    "  ],\n",
    "  \"bos_token_id\": 0,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 768,\n",
    "  \"model_type\": \"longformer\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"onnx_export\": False,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"sep_token_id\": 2,\n",
    "  \"transformers_version\": \"4.36.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2458e92d-5cfb-497d-a57e-d44be3905aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf =LongformerConfig(attention_window = 36, hidden_size =270, intermediate_size = 1024,num_hidden_layers =4, num_attention_heads =8, max_position_embeddings=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ffe8a04-d901-417c-bc97-9066b52b6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_layer =LongformerLayer(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8138499-3da9-4dd3-941e-5e8606cbc057",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LongformerLayer' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlong_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bm/lib/python3.8/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LongformerLayer' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "long_layer.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d442d6c-02ff-4ae2-b542-17aac252ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch =torch.rand(size=(256,1024,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f5c7cd2-13f0-4984-b245-0126df08709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1024, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d24bc27a-6f41-4f1c-bc5f-6f0eadec739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask =torch.ones(batch.size()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d841d491-5c20-4724-8235-2b0331cc6712",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_extended_attention_mask(\n",
    "        attention_mask: Tensor, input_shape: Tuple[int], device: torch.device = None, dtype: torch.float = None\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        dtype = model.dtype\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            \n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
    "        return extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98f5288e-f52b-4fd7-b348-a864232b852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0*torch.finfo(model.dtype).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce70bee2-5737-46e3-9249-1fe2cef99eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_att =get_extended_attention_mask(attention_mask=attention_mask, input_shape=batch.size()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb587bda-5cb4-4b60-9dcd-bd24dba7eb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_att[\n",
    "            :, 0, 0, :\n",
    "        ]> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11b8e419-30a1-447e-a9ec-ded8eac7820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_index_masked = attention_mask < 0\n",
    "is_index_global_attn = attention_mask < 0\n",
    "is_global_attn = is_index_global_attn.flatten().any().item()\n",
    "output_attentions=False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61f50a8a-d9ca-4e6b-a65a-442235f17127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1024])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_index_global_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c00281f-f450-4478-824c-7f505b8b276a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_global_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eddd1bb8-0aaf-4628-baeb-a1b9b1f38762",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = long_layer(\n",
    "                    batch,\n",
    "                    attention_mask=attention_mask,\n",
    "                    layer_head_mask=None,\n",
    "                    is_index_masked=is_index_masked,\n",
    "                    is_index_global_attn=is_index_global_attn,\n",
    "                    is_global_attn=is_global_attn,\n",
    "                    output_attentions=output_attentions,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44954f76-21ce-481b-ba24-7d1e66f2681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8e9f0d76-dd02-4c1c-a1bd-fa013ed2e34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1024, 768])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0f8ae694-3d1e-4f4b-9c43-b009a7e962df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 512, 12, 1025])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b5727ac-9928-4a17-9e14-648038aa894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 12, 512, 512])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0ffba55-c9c4-4a85-8479-c920211e5f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"onnx_export\": false,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e720d45-74cf-47ae-a00d-cf644590093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbcc080b-d4e5-46c1-853d-63b53307eeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5515568e784eab8a99fb0b1df28c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e318562f2ff48f9b776023b29f7f1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda5b63670de4b74a24e653430422361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e9f00dd1554e398d007144d6b18a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ef2994c-1834-4bf8-a21e-3158d41d97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4002 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d26cd644-44cb-463e-8de3-915b05a040a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4002])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394dbff-e5ee-4304-8272-4cf8eae1eda0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
